---
layout: page
nav_order: 2
---
# Modules

## Data consumer

Every time a new record is sent by Kafka to the subscribes topic, the
`KafkaConsumer` will handle and process it, recovering from the corresponding S3
bucket, and passing the downloaded file to the `Engine` in order to process it.

### Format of the received Kafka records

```json5
{
  "account": 123456, // (uint)
  "principal": 9, // (uint)
  "size": 55099, // (uint)
  "url": "https://insights-dev-upload-perm.s3.amazonaws.com/e927438c126040dab7891608447da0b5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJW4PUHKGSOIEEI7A%2F20200123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200123T161559Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=3e123beac8503f4338f611f85b053f7f15e69e2748228f9f98b6986e7c06fb6c", // (string)
  "b64_identity": "eyJlbnRpdGxlbWVudHMiOnsiaW5zaWdodHMiOnsiaXNfZW50aXRsZWQiOnRydWV9LCJjb3N0X21hbmFnZW1lbnQiOnsiaXNfZW50aXRsZWQiOnRydWV9LCJhbnNpYmxlIjp7ImlzX2VudGl0bGVkIjp0cnVlfSwib3BlbnNoaWZ0Ijp7ImlzX2VudGl0bGVkIjp0cnVlfSwic21hcnRfbWFuYWdlbWVudCI6eyJpc19lbnRpdGxlZCI6dHJ1ZX0sIm1pZ3JhdGlvbnMiOnsiaXNfZW50aXRsZWQiOnRydWV9fSwiaWRlbnRpdHkiOnsiaW50ZXJuYWwiOnsiYXV0aF90aW1lIjoxNDAwLCJvcmdfaWQiOiIxMjM4MzAzMiJ9LCJhY2NvdW50X251bWJlciI6IjYyMTIzNzciLCJhdXRoX3R5cGUiOiJiYXNpYy1hdXRoIiwidXNlciI6eyJmaXJzdF9uYW1lIjoiSW5zaWdodHMiLCJpc19hY3RpdmUiOnRydWUsImlzX2ludGVybmFsIjpmYWxzZSwibGFzdF9uYW1lIjoiUUUiLCJsb2NhbGUiOiJlbl9VUyIsImlzX29yZ19hZG1pbiI6dHJ1ZSwidXNlcm5hbWUiOiJpbnNpZ2h0cy1xZSIsImVtYWlsIjoiam5lZWRsZStxYUByZWRoYXQuY29tIn0sInR5cGUiOiJVc2VyIn19", // (string)
  "timestamp": "2020-01-23T16:15:59.478901889Z" // (string)
}
```

The attribute `b64_identity` contains another JSON encoded by BASE64 encoding.
User and org identities are stored here:

```json5
...
...
...
    "identity": {
        "account_number": "6212377",
        "auth_type": "basic-auth",
        "internal": {
            "auth_time": 1400,
            "org_id": "12383032"
        },
        "type": "User",
        "user": {
            "email": "jneedle+qa@redhat.com",
            "first_name": "Insights",
            "is_active": true,
            "is_internal": false,
            "is_org_admin": true,
            "last_name": "QE",
            "locale": "en_US",
            "username": "insights-qe"
        }
...
...
...

```

## Processing

The ICM `Engine` class takes the downloaded tarball and, using **ccx-ocp-core**
and **ccx-rules-ocp**, processes it and generates a JSON report. This report is
handled and sent to a configured Kafka topic using a `KafkaPublisher`.


## Reporting

The JSON report generated in the previous step is sent to a Kafka topic where
other services can take this record and handle it properly. Generated JSON has
format:

```json5
{
  "OrgID": 123456, // (int) - number that we get from b64_identity field
  "ClusterName": "aaaaaaaa-bbbb-cccc-dddd-000000000000", // (string) - cluster UUID  that we read from URL
  "Report": "{...}", // (string) - stringified JSON, that contains results of executing rules,
  "LastChecked": "2020-01-23T16:15:59.478901889Z" // (string) - time of the archive uploading in ISO 8601 format, gotten from "timestamp" field
}
```

The fields come from:

- `OrgID`: retrieved from the incoming JSON, codified inside the `b64_identity`
  value. It is extracted from `identity-internal-org_id` path of keys.
- `ClusterName`: the cluster name is retrieved from the downloaded archive. When
  the download successes and the archive is extracted prior to its processing by
  the engine, the cluster ID is read from a file named `config/id`.
- `Report`: is the JSON generated by the engine when the archive is processed.
- `LastChecked`: this field is copied directly from the incoming JSON, as
  `timestamp` key.

## Cluster name extraction internals

As the cluster ID is only present inside the archive and the set of rules to be
executed is configurable, this pipeline cannot relay on the executed rules to
get the cluster name.

For that reason, a mixed `Watcher` was created: `ClusterIdWatcher`, that
inherits from both `EngineWatcher` and `ConsumerWatcher`. This double
inheritance allows this watcher to receive notification from both entities of
the pipeline. The relevant events for this watcher are:

- `on_recv`: a consumer event used to store the `ConsumerRecord` object, needed
  to store the cluster name, when extracted
- `on_extract`: an engine event triggered when the archive is extracted, but not
  already processed. At this point, the files on the archive are available in
  the internal storage, so the relevant file can be read in order to store its
  content as cluster name.

## Prometheus statistics

The project allows to expose some metrics to **Prometheus** if desired. To
enable it, you should add the `ConsumerWatcher` to the configuration file, as
shown in the [provided one](config.yaml).

The exposed metrics are 6 counters and 3 histograms:

- `ccx_consumer_received_total`: a counter of the total amount of received
  messages from Kafka that can be handled by the pipeline.
- `ccx_downloaded_total`: total amount of handled messages that contains a valid
  and downloadable archive.
- `ccx_engine_processed_total`: total amount of archives processed by the
  Insights library.
- `ccx_published_total`: total amount of processed results that has been
  published to the outgoing Kafka topic.
- `ccx_failures_total`: total amount of individual events received but not
  properly processed by the pipeline. It can include failures due to an invalid
  URL for the archive, incorrect format of the downloaded archive, failure
  during the processing...
- `ccx_not_handled_total`: total amount of received records that cannot be
  handled by the pipeline, normally due to incompatible format or incorrect JSON
  schema.
- `ccx_download_duration_seconds`: histogram of the time that takes to download
  each archive.
- `ccx_process_duration_seconds`: histogram of the time that takes to process
  the archive after it has been downloaded.
- `ccx_publish_duration_seconds`: histogram of the time that takes to send the
  new record to the outgoing Kafka topic after the archive has been processed.

## Format of the logs

The log format is highly configurable through the configuration file. By
default, using the [provided configuration](config.yaml), each log message will
produce a JSON dictionary with the following structure:

```json5
{
  "levelname": "LOG_LEVEL",
  "asctime": "timestamp",
  "name": "Python module",
  "filename": "filename",
  "message": "Message content"
}
```

By default, this log messages will be printed in the standard output. To change
this behaviour, refer to the `logging` section in the [configuration
file](config.yaml),  the [Python Logging
HOWTO](https://docs.python.org/3.6/howto/logging.html#configuring-logging) and
the [Python logging
reference](https://docs.python.org/3.6/library/logging.config.html#module-logging.config)
